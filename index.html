<html>
<head>
<meta name="google-site-verification" content="WSTHKb10prs8dALCGozfSprSA5OBElE0l-0lXH69oTc" />
<meta name="description" content="The goal of this research is to explore how it is possible to optimize the communication channel between the learning agent's perception and the learning algorithm itself, so to exploit data at best, reduce the amount of learning steps, and improve performance.">
<meta name="keywords" content="reinforcement learning,temporal-difference learning,replay memory,prioritized experience replay, stochastic gradient descent,artificial neural networks,cognition,perception,meta-learning,q-learning,deep learning,evolutionary algorithms,cognitive modeling,intrinsic motivation,personality types,the big five personality traits,attention">

<link rel="preconnect" href="https://fonts.gstatic.com"> 
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@100;300;500&display=swap" rel="stylesheet">

<title>A Communication Channel View of Active Perception in Learning Agents</title>

<link rel="icon" 
      type="image/png" 
      href="favicon.ico" />

<style type="text/css">
	
body {

	font-family: 'Roboto', sans-serif;
    font-weight: 100;
    font-size: 15px;
}

p {
    font-weight: 100;
    font-size: 15px;
    text-align: justify;
}

h1,h3,a {
	font-family: 'Roboto', sans-serif;
    font-weight: 300;
}

#t {

	font-weight: 500;
	padding-bottom: 15px
}

a {

	font-size: 12px;
}

a:link,a:visited,a:active,a:hover {

	color: #000;
}


#margin
{
   width:660px;
   max-width:660px;
   margin:auto;
   padding:15px;
}

#gif {

	padding-left: 204px;
}

</style>

<script type="text/javascript">
function updateGIF() {
    el = document.getElementById("gif");

    el.src = "gif/" + (Math.floor(Math.random() * 29) + 1) + ".gif"
}
</script>


</head>

<body onload="updateGIF();">



<div id="margin">

<h3 id="t">A Communication Channel View of Active Perception in Learning Agents</h3>

<p>The goal of this research is to explore how it is possible to optimize the communication channel between the learning agent's perception and the learning algorithm itself, so to exploit data at best, reduce the amount of learning steps, and improve performance.</p>

<p>So-far obtained results show that it is possible to achieve this goal by selectively prioritizing the agent's experiences that are being effectivelly transfered via the communication channel. The techniques implemented range from minimization of the correlation between experiences in 1); Prioritization criteria based on type of reinforcement received in 2), 3) as well as the properties of agent's perceived space such as the Shannon's entropy in 4) and Kullback-Leibler divergence in 5); Evolutionary meta-learning applied to artificial neural network approximators as a selection mechanism has also been implemented on reinforcement learning problems that deal with a discrete and continuous state spaces in 6), 7).</p>

<h3>Motivation</h3>

<p>Online reinforcement learning agents are currently able to process an increasing amount of data by converting (approximating) it into a higher order reward functions such as values which are used to determine agents behaviour or policy.</p>
<p>The expansion of the information collected from the environment is mainly the product of the agent's trend to scale up to a class of more complex and realistic tasks that are quickly becoming a state-of-the-art benchmarks.</p>
<p>Bellman's curse of dimensionality warns us about the difficulties that approximation faces when scaling up to process high-dimensional data; With the increase of the dimension of the input the volume of that space (represented by all of its possible values) becomes exponentially higher leading to data sparsity.</p>
<p>To mitigate the effects of the data sparsity in an increasingly high dimensions the common approach is just to train it on a higher amount of data, which in reinforcement learning directly translates to the agent going through a computationally costly generation of more transitions over a specific environment.</p>

<h3>Shifting the Nature of the Problem</h3>

<p>If we take into consideration that, with the increased dimensionality of the data perceived by the agent we increase the sparsity of the collected data together with the fact that we are faced with a limited bandwidth between the agent observations and the learning mechanism we can look at presented challenges as a communication problem.</p>
<p>This is exactly the problem that Claude Shannon and John Tukey tried to solve during their period at the Bell Labs which inevitably led to the cornerstone of the information theory: the famous work by Shannon 8).</p>
<p>Sparsely encoded data at the input makes the source more predictable and thus the agent gains less information each time it receives data from its immediate environment. In other words, the sparse data that an agent perceives during it's interaction with the environment has lower Shannon's entropy.</p>
<p>The communication problem the two engineers faced in a nutshell is getting as much of information (Shannon's bits) through a channel of limited capacity measured by Tukey's bits. Theoretically the ideal communication case would be if the transferred information Shannon bits were equal to Tukeys: We would have used the full potential of the channels bandwidth.</p>
<p>This is definitely not the case with the research topic of machine learning applications: As we increase the dimensionality of the agent's input space the communication channel to the machine learning algorithm becomes less optimized.</p>


<img src="gif/1.gif" width="252" height="84" id="gif">


<h3>References</h3>

1) M Ramicic, A Bonarini, Uncertainty Maximization in Partially Observable Domains: A Cognitive Perspective, *Preprint

<br><a href="https://arxiv.org/abs/2102.11232">https://arxiv.org/abs/2102.11232</a>

<br><br>2) M Ramicic, A Bonarini, Augmented Memory Replay in Reinforcement Learning With Continuous Control

<br><a href="https://doi.org/10.1109/TCDS.2021.3050723">https://doi.org/10.1109/TCDS.2021.3050723</a>

<br><br>3) M Ramicic, A Bonarini, Correlation Minimizing Replay Memory in Temporal-Difference Reinforcement Learning

<br><a href="https://www.sciencedirect.com/science/article/pii/S092523122030179X">https://www.sciencedirect.com/science/article/pii/S092523122030179X</a>

<br><br>4) M Ramicic, A Bonarini, Attention-based experience replay in deep Q-learning

<br><a href="https://re.public.polimi.it/retrieve/handle/11311/1031983/226543/C111.pdf">https://re.public.polimi.it/retrieve/handle/11311/1031983/226543/C111.pdf</a>

<br><br>5) M Ramicic, A Bonarini, Adaptation of learning agents through artificial perception

<br><a href="https://re.public.polimi.it/retrieve/handle/11311/1119596/463799/AdaptationOfLearningAgentsThroughArtificialPerceptionPerception.pdf">https://re.public.polimi.it/retrieve/handle/11311/1119596/463799/AdaptationOfLearningAgentsThroughArtificialPerceptionPerception.pdf</a>

<br><br>6) M Ramicic, A Bonarini, Entropy-based prioritized sampling in deep Q-learning

<br><a href="https://re.public.polimi.it/retrieve/handle/11311/1031981/226539/V414.pdf">https://re.public.polimi.it/retrieve/handle/11311/1031981/226539/V414.pdf</a>

<br><br>7) M Ramicic, A Bonarini, Towards learning agents with personality traits: Modeling Openness to Experience

<br><a href="https://re.public.polimi.it/retrieve/handle/11311/1120069/464828/LearningAgentsWithPersonalityTraits.pdf">https://re.public.polimi.it/retrieve/handle/11311/1120069/464828/LearningAgentsWithPersonalityTraits.pdf</a>

<br><br>8) M Ramicic, A Bonarini, Selective perception as a mechanism to adapt agents to the environment: An evolutionary approach

<br><a href="https://re.public.polimi.it/retrieve/handle/11311/1120067/464823/SelctivePerception08630035.pdf">https://re.public.polimi.it/retrieve/handle/11311/1120067/464823/SelctivePerception08630035.pdf</a>

<br><br>9) M Ramicic, A Bonarini, Augmented Replay Memory in Reinforcement Learning With Continuous Control, *Preprint

<br><a href="https://arxiv.org/abs/1912.12719">https://arxiv.org/abs/1912.12719</a>

<br><br>10) C.E. Shannon, A mathematical theory of communication

<br><a href="https://ieeexplore.ieee.org/document/6773024">https://ieeexplore.ieee.org/document/6773024</a>

<h3>Keywords</h3>
reinforcement learning,temporal-difference learning,replay memory,prioritized experience replay, stochastic gradient descent,artificial neural networks,cognition,perception,meta-learning,q-learning,deep learning,evolutionary algorithms,cognitive modeling,intrinsic motivation,personality types,the big five personality traits,attention

<h3>More</h3>

<a href="https://www.researchgate.net/project/A-Communication-Channel-View-of-Active-Perception-in-Learning-Agents">https://www.researchgate.net/project/A-Communication-Channel-View-of-Active-Perception-in-Learning-Agents</a>

<br><br><a href="https://bonarini.faculty.polimi.it">https://bonarini.faculty.polimi.it</a>

<br><br><a href="https://www.aic.fel.cvut.cz">https://www.aic.fel.cvut.cz</a>

<br><br>

</div>

</body>
</html>